{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads ERA40 and WATCH focing data for GRAND dams inflow forecast\n",
    "- ERA40: Volumetric Soil Water (SWVL)\n",
    "- WATCH: Snowfall (Snowf)\n",
    "\n",
    "Donghoon Lee @ May-23-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import num2date, Dataset\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import gdal\n",
    "import snetk\n",
    "import datetime\n",
    "from tools import DownloadFromURL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ERA40 Volumetric Soil Water (SWVL) Layer 1-4\n",
    "The unit of SWVL is $m^3m^{-3}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filn_swvl = '/Users/dlee/data/era/swvl_mon.nc'\n",
    "if not os.path.exists(filn_swvl):\n",
    "    # Load NetCDF files\n",
    "    ds = xr.open_mfdataset('/Users/dlee/data/era/swvl/era_sm_*.nc')\n",
    "    # Sums 4 layers\n",
    "    ds_month = ds.swvl1 + ds.swvl2 + ds.swvl3 + ds.swvl4\n",
    "    ds_month.name = 'swvl'\n",
    "    # Selecting time period from 1958-01 to 2000-12\n",
    "    swvl_mon = ds_month.loc['1958-01':'2000-12']\n",
    "    if False:\n",
    "        # Moving average (seasonal mean)\n",
    "        ds_season = ds_month.rolling(time=3, center=True).mean().dropna('time')\n",
    "        ds_season.name = 'swvl'\n",
    "        swvl_mo3 = ds_season.loc['1958-01':'2000-12']\n",
    "    # Save to NetCDF file\n",
    "    swvl_mon.to_netcdf(filn_swvl)\n",
    "    print('%s is saved.' % filn_swvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading WATer and global CHange (WATCH) Forcing Data (WFD) - 20th Century data: Snowfall (Snowf)\n",
    "The original (time resoultion is second) unit of Snowf is $kg m^{-2}s^{-1}$. This is aggregated at monthly scale, so the final unit is $kgm^{-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filn_snow = '/Users/dlee/data/watch/snowf_mon.nc'\n",
    "if not os.path.exists(filn_snow):\n",
    "    # Load NetCDF files\n",
    "    ds = xr.open_mfdataset('/Users/dlee/data/watch/snowf/Snowf_WFD_GPCC_*.nc', decode_times=False, decode_cf=True)\n",
    "    # Fix the DatetimeIndex of the Xarray Dataset\n",
    "    time_fixed = pd.date_range('1958-01-01 00:00:00', periods=ds.time.shape[0], freq='3H')\n",
    "    ds = ds.drop(['time','timestp','nav_lon','nav_lat'])\n",
    "    ds = ds.assign_coords(time = time_fixed)\n",
    "    ds = ds.rename_dims({'land':'land', 'tstep':'time'})\n",
    "    # Resample\n",
    "    snow_mon = ds.resample(time=\"1M\").sum()\n",
    "    # Save to NetCDF file\n",
    "    snow_mon.to_netcdf(filn_snow)\n",
    "    print('%s is saved.' % filn_snow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "- ERA40 longitude starts from 0 degree, so we shift the western half to the east.\n",
    "- Reshape to global 2D data (516 tims, 259200 grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshaping ERA40-swvl data\n",
    "# Loading NetCDF file\n",
    "nc = Dataset(filn_swvl, 'r')\n",
    "swvl = np.array(nc.variables['swvl'])\n",
    "lat = np.array(nc.variables['latitude'])\n",
    "lon = np.array(nc.variables['longitude'])\n",
    "tim = nc.variables['time']\n",
    "tim = num2date(tim[:], tim.units, tim.calendar)\n",
    "# Shift the western half to the east\n",
    "swvl = np.dstack((swvl[:,:,360::], swvl[:,:,0:360]))\n",
    "swvl = swvl[:,:-1,:]\n",
    "lon = np.hstack((lon[360::] - 360, lon[0:360]))\n",
    "swvlCopy = swvl.copy()\n",
    "# Make ERA40-swvl land-mask\n",
    "lsm_swvl = ~(swvl[1,:,:] == np.min(swvl[1,:,:])) + 0\n",
    "# Reshape to 2D format\n",
    "swvl = swvl.reshape([swvl.shape[0], swvl.shape[1]*swvl.shape[2]])\n",
    "\n",
    "## Reshaping WFD-snowf data\n",
    "# Loading NetCDF file\n",
    "nc = Dataset(filn_snow, 'r')\n",
    "snow_raw = np.array(nc['Snowf'])\n",
    "land = np.array(nc['land'])\n",
    "# Reshape to 2D format\n",
    "snow = np.zeros([516, 360*720])\n",
    "snow[:,np.int32(land)-1] = snow_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data in ocean area\n",
    "While we calculate summation and rolling averages in 'Xarray', the missing values have been changed.</br>\n",
    "Here, we will find grids with approximately zero (probabily NaN) values, then set those grids as missing data (zero).</br>\n",
    "There still are some missing values (e.g.,negative), but we ignore first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ERA40 SWVL\n",
    "# Find approximately zero\n",
    "rdx_swvl = (np.sum(np.abs(swvl) < 1e-05, axis=0) == 516)     # 97270 grids alive\n",
    "# Replace those grids to zero\n",
    "swvl[:,rdx_swvl] = 0\n",
    "\n",
    "## WFD Snowf\n",
    "rdx_snow = np.setdiff1d(np.arange(0,360*720), np.int32(land)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basin-scale accumulated data using DDM30 upsteam network\n",
    "Here, we accumulate the data using global streamflow network (DDM30) and flow accumulation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load streamflow network (DDM30)\n",
    "with rasterio.open('./data/DDM30_fdir_rec.tif') as src:\n",
    "    fdir = src.read(1)\n",
    "# Aggregate upstream swvl\n",
    "filn_swvl_up = '/Users/dlee/data/era/swvl_mon_up_ddm30.npz'\n",
    "filn_snow_up = '/Users/dlee/data/watch/snowf_mon_up_ddm30.npz'\n",
    "if not (os.path.exists(filn_swvl_up) & os.path.exists(filn_snow_up)):\n",
    "    # Create aggregated upstream soil moisture\n",
    "    swvl_up = np.zeros(swvl.shape)\n",
    "    snow_up = np.zeros(snow.shape)\n",
    "    for i in range(swvl_up.shape[0]):\n",
    "        # Convert table format to map format\n",
    "        swvl_map = np.reshape(swvl[i,:], [360,720])\n",
    "        snow_map = np.reshape(snow[i,:], [360,720])\n",
    "        # Remove the latitudes where the flow direction does not exist\n",
    "        # *12 and 292 are manually checked\n",
    "        swvl_map = swvl_map[12:292,:] \n",
    "        snow_map = snow_map[12:292,:] \n",
    "        # Accumulate soil moisture with flow accumulation\n",
    "        swvl_acc = snetk.flowAccumulate(fdir, swvl_map)\n",
    "        snow_acc = snetk.flowAccumulate(fdir, snow_map)\n",
    "        # Add the latitudes for being original size\n",
    "        swvl_acc = np.vstack((np.zeros([12,720]), swvl_acc, np.zeros([68,720])))\n",
    "        snow_acc = np.vstack((np.zeros([12,720]), snow_acc, np.zeros([68,720])))\n",
    "        # Convert map format to table format\n",
    "        swvl_up[i,:] = np.reshape(swvl_acc, [1,360*720])\n",
    "        snow_up[i,:] = np.reshape(snow_acc, [1,360*720])\n",
    "        print(\"%d/%d is processed..\" % (i+1, swvl_up.shape[0]))\n",
    "        \n",
    "    # Remove the identify NaN grids\n",
    "    swvl_up[:,rdx_swvl] = 0\n",
    "    snow_up[:,rdx_snow] = 0\n",
    "    # Save the accumulated upstream soil moisture\n",
    "    np.savez_compressed(filn_swvl_up, swvl_up=swvl_up)\n",
    "    print('%s is saved.' % filn_swvl_up)\n",
    "    np.savez_compressed(filn_snow_up, snow_up=snow_up)\n",
    "    print('%s is saved.' % filn_snow_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data at GRAND Dams Grids\n",
    "Here, we select data at the grids that 1,593 GranD Dams located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accumulated upstream soil moisture\n",
    "swvl_up = np.load(filn_swvl_up)['swvl_up']\n",
    "snow_up = np.load(filn_snow_up)['snow_up']\n",
    "\n",
    "# Load the selected 1,593 GranD Dams\n",
    "ind_dams = np.load('./data/ind_dams.npz')['ind_dams']\n",
    "damList = ind_dams[0,:]\n",
    "ind_dams = ind_dams[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select swvl_up values at the 1,593 dam locations\n",
    "swvl_up_dams = swvl_up[:,ind_dams]\n",
    "snow_up_dams = snow_up[:,ind_dams]\n",
    "# Check grids with all 0 (NaN) values (3 points are found)\n",
    "no_swvl = (np.sum(swvl_up_dams < 1e-05, axis=0) > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dfSwvlDams.hdf is saved.\n",
      "./data/dfSnowDams.hdf is saved.\n"
     ]
    }
   ],
   "source": [
    "# Save as Pandas DataFrame\n",
    "mondex = pd.period_range('{:04d}-{:02d}'.format(1958,1), periods=516, freq='M')\n",
    "dfSwvlDams = pd.DataFrame(swvl_up_dams, index=mondex, columns=damList)\n",
    "dfSnowDams = pd.DataFrame(snow_up_dams, index=mondex, columns=damList)\n",
    "filn = './data/dfSwvlDams.hdf'\n",
    "dfSwvlDams.to_hdf(filn, key='df', complib='blosc:zstd', complevel=9); print('%s is saved.' % filn)\n",
    "filn = './data/dfSnowDams.hdf'\n",
    "dfSnowDams.to_hdf(filn, key='df', complib='blosc:zstd', complevel=9); print('%s is saved.' % filn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land-mask comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compared to PCRGLOB-WB's global coverage, ERA40-swvl has missing values in \n",
    "# # some areas (e.g., Central Africa dessert, the Great lakes, etc.)\n",
    "# #\n",
    "# # Load ERA40 land-mask\n",
    "# nc = Dataset(os.path.join('data', 'era', 'lsm.nc'), 'r')\n",
    "# lsm_era = np.array(nc.variables['lsm']).astype(np.int16)\n",
    "# lsm_era = np.squeeze(lsm_era); lsm_era = lsm_era[:-1,:]\n",
    "# lsm_era = np.hstack((lsm_era[:,360::], lsm_era[:,0:360]))\n",
    "# # Load PCRGLOB-WB land-mask\n",
    "# lsm_pcr = np.load('./data/landMask_PCRGLOBWB.npy')      # From discharge data\n",
    "# lsm_pcr[lsm_pcr != 1] = 0\n",
    "# # Load WATCH land-mask\n",
    "# nc = Dataset(os.path.join('data','watch','SnowCRs-land.nc'))\n",
    "# ldc_wat = np.array(nc.variables['land'])\n",
    "# lsm_wat = np.zeros([360,720])\n",
    "# lsm_wat[np.unravel_index(ldc_wat-1, [360,720], order='C')] = 1\n",
    "# # Generate map of dam locations (as grids)\n",
    "# lsm_dams = np.zeros([360,720]).astype('int16')\n",
    "# lsm_dams[np.unravel_index(ind_dams, [360,720])] = 1     # Total 1,261 grids    \n",
    "\n",
    "# # Number of land grids\n",
    "# print('=============================')\n",
    "# print('Number of land grids')\n",
    "# print('-----------------------------')\n",
    "# print('ERA40:\\t\\t{:,}'.format(np.sum(lsm_era == 1)))        # 86,840\n",
    "# print('ERA40-swvl:\\t{:,}'.format(np.sum(lsm_swvl == 1)))    # 97,247\n",
    "# print('WATCH:\\t\\t{:,}'.format(np.sum(lsm_wat == 1)))        # 67,420\n",
    "# print('PCRGLOB-WB:\\t{:,}'.format(np.sum(lsm_pcr == 1)))     # 66,273\n",
    "# print('1,593 dams:\\t{:,}'.format(np.sum(lsm_dams == 1)))    # 1,261\n",
    "# print('=============================')\n",
    "# # =============================================================================\n",
    "# # # Create GeoTiff maps\n",
    "# # import gdal\n",
    "# # import snetk\n",
    "# # lsm_pcr = lsm_pcr.astype('int16')\n",
    "# # out_ds = snetk.make_raster360('lsm_pcr.tif', lsm_pcr, gdal.GDT_Int16, nodata=0); del out_ds\n",
    "# # out_ds = snetk.make_raster360('lsm_era.tif', lsm_era, gdal.GDT_Int16, nodata=0); del out_ds\n",
    "# # out_ds = snetk.make_raster360('lsm_swvl.tif', lsm_swvl, gdal.GDT_Int16, nodata=0); del out_ds\n",
    "# # out_ds = snetk.make_raster360('lsm_dams.tif', lsm_dams, gdal.GDT_Int16, nodata=0); del out_ds\n",
    "# # =============================================================================\n",
    "\n",
    "\n",
    "# #%% Correlation between streamflow and soil moisture datasets\n",
    "# # Two soil moisture datasets are considered:\n",
    "# # - (a) grid specific soil moisture\n",
    "# # - (b) accumulated upstream soil moisture\n",
    "# from scipy.stats import pearsonr\n",
    "# filnFlow = Path('/home/dlee/gdrive/gflood/data/flow.npy')\n",
    "# flow = np.load(filnFlow).transpose()\n",
    "# corrTable = np.zeros([flow.shape[1], 4])\n",
    "# for i in range(flow.shape[1]):\n",
    "#     corrTable[i,0:2] = pearsonr(flow[:,i], swvl[:,ind_pcr[i]])\n",
    "#     corrTable[i,2:] = pearsonr(flow[:,i], swvl_up[:,ind_pcr[i]])\n",
    "# # Print results\n",
    "# print('-'*30)\n",
    "# print('Number of significant grids')\n",
    "# print('swvl:\\t\\t{:,}'.format(np.sum(corrTable[:,1] < 0.05)))\n",
    "# print('swvl_up:\\t{:,}'.format(np.sum(corrTable[:,3] < 0.05)))\n",
    "# print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download WATer and global CHange (WATCH) Forcing Data (WFD) - 20th Century data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WATCH Forcing data\n",
    "# path_local = '/Users/dlee/data/watch/snowf/'\n",
    "# path_watch = 'https://catalogue.ceh.ac.uk/datastore/eidchub/6b7a08f0-00f7-4c48-ac4d-e483acb77a03/extracted/'\n",
    "# dti = pd.date_range('1958-01', '2001-1', freq='M') # 516\n",
    "# localList = [path_local+'Snowf_WFD_GPCC_%s.nc.gz' % time for time in dti.strftime('%Y%m')]\n",
    "# remoteList = [path_watch+'Snowf_WFD_GPCC_%s.nc.gz' % time for time in dti.strftime('%Y%m')]\n",
    "# DownloadFromURL(remoteList, localList, showLog=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
